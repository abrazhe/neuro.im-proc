---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.13.7
  kernelspec:
    display_name: venv
    language: python
    name: venv
---

```{python}
# %pylab inline
```

```{python}
import sys
import ccdb
import napari
import hessian_cecp as hcecp

from importlib import reload

from tqdm.auto import tqdm
from scipy import ndimage as ndi


from imfun import fseq
```

```{python}
from astromorpho import io as aio
from astromorpho import enh, morpho
```

```{python}
rc('image', aspect='equal', interpolation='nearest',cmap='gray')
rc('figure', figsize=(10,10))
```

```{python}
rc('axes', grid=True, labelsize=16)
```

```{python}
data_path = '/home/incredible/Neuroscience/selected-for-complexity/data'
```

```{python}
# cd $data_path
```

```{python}
#sys.path.append('/home/brazhe/proj/semyanov-data-gitlab/')
sys.path.append('/home/incredible/Neuroscience/lib')
```

```{python}
# names = !ls *.pic
names = [name for name in names if not 'hm' in name]
# names
```

```{python}
len([n for n in names if 'both' in n])
```

```{python}
reload(hcecp)
```

## Test Data

```{python}
def gaussian_blob(x,y, sigma_x, sigma_y):
    return np.exp(-x**2/(2*sigma_x**2) - y**2/(2*sigma_y**2))

X,Y = mgrid[:256,:256]

#sigma_r_v = logspace(-1, 1.6, 16, base=2)
sigma_r_v = ones(1000)

locs = arange(15,250,15)
print(len(locs))


centers = [(r,c) for r in locs for c in locs]

img_x = np.sum([gaussian_blob(X-r,Y-c, sigma_r, sigma_r) for (r,c),sigma_r in zip(centers, sigma_r_v)],0)
```

```{python}
figure()
imshow(img_x)
```

```{python}
test_stack =  np.array([img_x]*100)
test_stack = test_stack + randn(*test_stack.shape)*0.005
```

```{python}
#test_stack = moveaxis(test_stack, 0,2)
```

```{python}
test_stack.dtype
```

```{python}
test_stack.shape
```

```{python}
# #%matplotlib qt
# %gui qt
```

```{python}
test_stack = test_stack.astype(float32)
```

```{python}
napari.view_image(test_stack, ndisplay=3)
```

```{python}
# %time test_sato, Vf = hcecp.sato3d(test_stack, 3, return_vectors=True)
```

```{python}
napari.view_image(test_sato, ndisplay=3)
```

```{python}
reload(hcecp)
```

```{python}
# %time hc = hcecp.hessian_cecp3d(test_sato, 3, with_plot=True)
```

```{python}
hc
```

## Real cell

```{python}
#k = 18
#k = randint(len(names))
k = 48

print(k, names[k])
stack, meta = ccdb.read_pic(names[k])
dims = ccdb.get_axes(meta)#[::-1]
dims
```

```{python}
if len(dims):
    zoom = dims[0][0]/dims[-1][0]
else:
    zoom = 4
```

```{python}
1/zoom
```

```{python}
stackz = ndi.zoom(stack.astype(float),(1, zoom,zoom))
```

```{python}
stack.shape, stackz.shape
```

```{python}
imshow(stackz.max(0))
```

```{python}

```

```{python}
w = napari.view_image(stackz, ndisplay=3, rendering='attenuated_mip', gamma=0.6)
```

```{python}
#imshow(w.screenshot())
#grid(False)
```

```{python}
# %time data_sato = hcecp.sato3d(stackz, 1, return_vectors=False)
napari.view_image(data_sato, ndisplay=3, rendering='attenuated_mip', gamma=0.6)
```

```{python}
# %time stackz_sato = hcecp.percentile_rescale(data_sato)
napari.view_image(stackz_sato, ndisplay=3, rendering='attenuated_mip', gamma=0.6)
```

```{python}
tmp = stackz_sato - data_sato
napari.view_image(tmp, ndisplay=3, rendering='attenuated_mip', gamma=0.6)
```

```{python}

```

```{python}
from ucats.patches import make_grid
```

```{python}
#make_grid(stackz.shape, 10,1)
```

```{python}
w = napari.view_image(stackz, ndisplay=3, rendering='attenuated_mip')
w.add_image(stackz_sato, colormap='red', gamma=0.75, opacity=0.5)
```

```{python}
# imshow(w.screenshot())
# grid(False)
```

```{python}
#from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import
```

```{python}
stackz.shape
```

```{python}
# %time hc = hcecp.hessian_cecp3d(stackz, 3, nbins=25, with_plot=True)
hc
```

```{python}
reload(hcecp)
```

```{python}
# can do a 3D map then? Show complexity in 3D as overlay over 3D morphology?
```

```{python}
# for illustrations
#patch_size=18
#patch_overlap = 16

# patch_size=24
# patch_overlap = 12

patch_size = 48
patch_overlap = 24
```

```{python}

```

```{python}
24*0.2, 48*0.2
```

```{python}

# #%time hc_acc = hcecp.hessian_cecp3d(randn(*stackz.shape), 3, spatial_binning=patch_size,spatial_overlap=patch_overlap)
# %time hc_acc = hcecp.hessian_cecp3d(stackz, 3, spatial_binning=patch_size,spatial_overlap=patch_overlap)
```

```{python}
hc_mf = np.sum(hc_acc[:,:2]*hc_acc[:,2][:,None], axis=0)
hc_mf
```

```{python}
stackz.shape
```

```{python}
cstack = np.zeros(stackz.shape)
hstack = np.zeros(stackz.shape)

windows = hcecp.make_grid(stackz.shape, patch_size, patch_overlap)
counts = np.zeros(stackz.shape,int)

for w, hc in zip(windows, hc_acc):
    hstack[w] += hc[0]
    cstack[w] += hc[1]
    counts[w] += 1
    
cstack = cstack/(1e-5 + counts)
hstack = hstack/(1e-5 + counts)
```

```{python}
len(hc_acc), len(windows)
```

```{python}
cstack.shape, stackz.shape
```

```{python}
w = napari.view_image(stackz, ndisplay=3,colormap='red',gamma=0.75)
w.add_image(cstack,blending='additive',colormap='green',opacity=0.5)
#w.add_image(hstack,blending='additive',colormap='gray_r')
```

```{python}

```

```{python}
ax = hcecp.prep_hc_axes(50*50)
ax.scatter(hc_acc[:,0],hc_acc[:,1], s=2, c=hc_acc[:,2],alpha=0.5,cmap='Reds')
plot(hc_mf[0],hc_mf[1],'r+',ms=25,mew=3,)
```

```{python}
# ax = hcecp.prep_hc_axes(50*50)
# ax.scatter(hc_acc[:,0],hc_acc[:,1], s=2, c=hc_acc[:,2],alpha=0.5,cmap='Reds')
# plot(hc_mf[0],hc_mf[1],'r+',ms=25,mew=3,)
```

```{python}

```

```{python}
# %time hc = hcecp.hessian_cecp3d(stackz, 3,  with_plot=True)
hc
```

```{python}
sigmas = array((0.5, 1,2,4,8,))
```

```{python}
hcx = array([hcecp.hessian_cecp3d(stackz, s) for s in tqdm(sigmas)])
```

```{python}
ax = hcecp.prep_hc_axes(50*50)

scatter(hcx[:,0],hcx[:,1],c=sigmas,cmap='rainbow');colorbar()
plot(hcx[:,0],hcx[:,1],'-')

grid(False)
```


### Collecting for all cells

```{python}
len(names)
```

```{python}
dim_acc = []
for name in names:
    stack_, meta_ = ccdb.read_pic(name)
    dims_ = ccdb.get_axes(meta_)#[::-1]
    dim_acc.append(dims_)
    

```

```{python}
k = 7
names[k], ccdb.get_axes(ccdb.read_pic(names[k])[1])
```

```{python}
[name for name in names if not  ccdb.get_axes(ccdb.read_pic(name)[1])]
```

```{python}
names_with_scale = [name for name in names if  ccdb.get_axes(ccdb.read_pic(name)[1])]
```

```{python}

dim_acc = np.array([(d[1][0], d[2][0], d[2][0]/d[1][0]) for d in tqdm(dim_acc) if len(d)])
```

```{python}

```

```{python}
#dim_acc
```

```{python}
from scipy import stats
```

```{python}
# data = dim_acc[:,0]

# hist(data,20);

# pdf = stats.kde.gaussian_kde(data)
# x = linspace(min(data),max(data),100)
# mode = x[argmax(pdf(x))]
# #m = stats.mode(dim_acc[:,0],)
# axvline(mode, color='tomato', ls = '--')
# print(mode)
```

```{python}
data = dim_acc[:,1]

hist(data,20);

pdf = stats.kde.gaussian_kde(data)
x = linspace(min(data),max(data),100)
mode = x[argmax(pdf(x))]
#m = stats.mode(dim_acc[:,0],)
axvline(mode, color='tomato', ls = '--')
print(mode)
```

```{python}
target_scale_ = mode # um/px
```

```{python}
target_scale_
```

```{python}
target_scale = 0.2
```

```{python}
def load_stack_with_rescale(name, target_scale=0.2,verbose=False):
    if verbose:
        print(name)
    stack_, meta_ = ccdb.read_pic(name)
    dims_ = ccdb.get_axes(meta_)#[::-1]
    dx, dz = dims_[1][0], dims_[2][0]
    if verbose:
        print(dx,dz)
        print(dx/target_scale, dz/target_scale)
    stackz = ndi.zoom(stack_.astype(float),(dz/target_scale, dx/target_scale,dx/target_scale), order=1)
    return stackz
```

```{python}
name = names_with_scale[randint(len(names_with_scale))]
stackz = load_stack_with_rescale(name, verbose=True)
```

```{python}
napari.view_image(stackz, ndisplay=3)
```

```{python}
hc_acc.shape
```

```{python}

def get_cell_complexity(stack, sigmas=(0.5, 1, 2, 4, 8, 16), patch_size=24, patch_overlap=12):
    out = {}
    for s in sigmas:
        hc_acc = hcecp.hessian_cecp3d(stack, s, spatial_binning=patch_size,spatial_overlap=patch_overlap)
        hc_mf = np.sum(hc_acc[:,:2]*hc_acc[:,2][:,None], axis=0)
        out[s] = hc_mf
    return out
    
```

```{python}
import pandas as pd
```

```{python}
reload(hcecp)
```

```{python}
2**arange(-1,4,0.5)
```

```{python}
sigmas_new = np.round(logspace(-1,4,12, base=2),3)
#sigmas_new = 2**arange(-1,4,0.25)
sigmas_new
```

```{python}
# %time res12 = get_cell_complexity(stackz,sigmas=sigmas_new,patch_size=12,patch_overlap=6)
# %time res24 = get_cell_complexity(stackz,sigmas=sigmas_new,patch_size=24,patch_overlap=12)
# %time res32 = get_cell_complexity(stackz,sigmas=sigmas_new,patch_size=32,patch_overlap=16)
# %time res48 = get_cell_complexity(stackz,sigmas=sigmas_new,patch_size=48,patch_overlap=24)
```

```{python}
df12 = pd.DataFrame(res12, index=('h','c'))
df24 = pd.DataFrame(res24, index=('h','c'))
df32 = pd.DataFrame(res32, index=('h','c'))
df48 = pd.DataFrame(res48, index=('h','c'))
```

```{python}
ax = hcecp.prep_hc_axes(50*50)
plot(df12.loc['h'], df12.loc['c'], label='12')
plot(df24.loc['h'], df24.loc['c'], label='24')
plot(df32.loc['h'], df32.loc['c'], label='32')
plot(df48.loc['h'], df48.loc['c'], label='48')
legend()
```

```{python}
import pickle
import seaborn as sns
import os
```

```{python}
shapes = array([load_stack_with_rescale(name).shape for name in tqdm(names_with_scale)])
shapes.mean(0)
```

```{python}
name_test = names_with_scale[randint(len(names_with_scale))]
stack_test = load_stack_with_rescale(name)
```

```{python}
stack_test.shape
```

```{python}
napari.view_image(stack_test)
```

```{python}
stack_randomized = permutation(ravel(stack_test)).reshape(stack_test.shape)
```

```{python}
napari.view_image(stack_randomized)
```

```{python}
res_r = {}

for ps in tqdm((12, 16, 24, 32, 48, 64)):
    res_r[ps] = get_cell_complexity(stack_randomized,sigmas=sigmas_new,patch_size=ps,patch_overlap=ps//2)
```

```{python}
res_r[12]
```

```{python}
fig, axs = subplots(2,1, sharex=True)
for ps in tqdm((12, 16, 24, 32, 48, 64)):
    d = pd.DataFrame(res_r[ps], index=('h','c'))
    axs[0].plot(d.loc['c'], label=ps)
    axs[1].plot(d.loc['h'],label=ps)
legend()
```

```{python}
d
```

```{python}

```

```{python}
def collect_complexities(patch_size,sigmas,randomized=False,need_rebuild=False):
    
    if not randomized:
        out_file = f'outputs/hessian_cecps_results-new_patch{patch_size}.pickle'
    else:
        out_file = f'outputs/hessian_cecps_results-new_patch{patch_size}-randomized.pickle'

    if os.path.exists(out_file) and not need_rebuild:
        hc_results = pickle.load(open(out_file,'rb'))
    else:
        hc_results = []
        for name in tqdm(names_with_scale):
            stack = load_stack_with_rescale(name)
            if randomized:
                stack = permutation(ravel(stack)).reshape(stack.shape)
            res = get_cell_complexity(stack,sigmas=sigmas,patch_size=patch_size,patch_overlap=patch_size//2)
            hc_results.append((name,res))
        pickle.dump(hc_results, open(out_file,'wb'))
    
    wk1_results = [pd.DataFrame(r[1], index=('h','c')) for r in hc_results if '1wk' in r[0]]
    wk3_results = [pd.DataFrame(r[1], index=('h','c')) for r in hc_results if '3wk' in r[0]]
    wk4_results = [pd.DataFrame(r[1], index=('h','c')) for r in hc_results if '4wk' in r[0]]
    
    wk1_H = pd.DataFrame([r.loc['h'] for r in wk1_results],index=range(len(wk1_results)))
    wk1_C = pd.DataFrame([r.loc['c'] for r in wk1_results],index=range(len(wk1_results)))
    
    wk3_H = pd.DataFrame([r.loc['h'] for r in wk3_results],index=range(len(wk3_results)))
    wk3_C = pd.DataFrame([r.loc['c'] for r in wk3_results],index=range(len(wk3_results)))
    
    wk4_H = pd.DataFrame([r.loc['h'] for r in wk4_results],index=range(len(wk4_results)))
    wk4_C = pd.DataFrame([r.loc['c'] for r in wk4_results],index=range(len(wk4_results)))
    
    dr = pd.DataFrame(res_r[patch_size], index=('h','c'))
    #print(dr)
    
    colors = ('green','royalblue', 'tomato')
    
    fig, axs = subplots(2,3, sharey='row',sharex=True, figsize=(18,8),
                        gridspec_kw=dict(hspace=0.01,wspace=0.05,))
    for ax, data, color in zip(axs[0], [wk1_C, wk3_C, wk4_C], colors):
        if ax is axs[0,0]:
            ax.set_ylabel('Hessian complexity')
        sns.stripplot(data=data,size=5,color=color,linewidth=1,alpha=0.5,ax=ax)
        sns.boxplot(data=data,color='white',linewidth=3, width=0.25,ax=ax)
        #ax.plot(array(dr.loc['c']), color='skyblue', lw=3, alpha=0.5)

    
    
    
    #fig, axs = subplots(1,3, sharey=True, figsize=(18,6))
    #scales = array(sigmas)*target_scale
    for ax, data, color in zip(axs[1], [wk1_H, wk3_H, wk4_H], colors):
        if ax is axs[1,0]:
            ax.set_ylabel('Hessian entropy')
        sns.stripplot(data=data,size=5,color=color,linewidth=1,alpha=0.5,marker='d',ax=ax)
        sns.boxplot(data=data,color='white',linewidth=3, width=0.25,ax=ax)
        #ax.plot(array(dr.loc['h']),  color='skyblue', lw=3, alpha=0.5)
        #ax.set_xticklabels([f'{s*target_scale :1.2f}' for s in log2(sigmas_new)])
        ax.set_xticklabels([f'{s*target_scale :1.2f}' for s in sigmas])
        ax.set_xlabel('spatial scale um ')
    #tight_layout()
    #savefig('outputs/ccdb-hessian-complexity-entropy-patch=24-scale=0.2um-rand.png')
    #savefig('outputs/ccdb-hessian-complexity-entropy-patch=24-scale=0.2um.svg')
    savefig(f'outputs/ccdb-hessian-complexity-patch={patch_size}-scale=0.2um-rand:{randomized}.png')
    savefig(f'outputs/ccdb-hessian-complexity-patch={patch_size}-scale=0.2um-rand:{randomized}.svg')
    
    return hc_results
```

```{python}
target_scale
```

```{python}
64*0.2, 32*0.2, 16*0.2
```

```{python}
from ucats import masks as umasks
```

```{python}
import itertools as itt

def locations(shape):
    """ all locations for a shape; substitutes nested cycles
    """
    return itt.product(*map(range, shape))


def img2points(img, th=0):
    "Converts a mask to a list of points, as row,col"
    points = []
    for loc in locations(img.shape):
        if img[loc] >= th:
            points.append(loc + (img[loc],))
    return points

def img_center(img, th=0, gamma=1):
    pts = array(img2points(img, th))
    print(pts.shape)
    weights = pts[:,-1]**gamma
    weights = weights/(1e-6 + np.sum(weights))
    return np.sum(pts[:,:-1]*weights[:,None], axis=0)


def add_scalebar(ax,length=25, height=1,scale=0.1,xy=None,unit='Î¼m',color='w'):
    l = length/scale
    h = height/scale
    setp(ax, xticks=[],yticks=[],frame_on=False)
    if xy is None:
        sh = ax.images[0].get_size()
        x = sh[1] - l - 0.05*sh[1]
        y = sh[0] - h - 0.05*sh[0]
        xy= x,y
    r = Rectangle(xy,l,h, color=color )
    ax.text(xy[0]+l/2,xy[1],s='{} {}'.format(length,unit),color=color,
            fontsize=18,
            horizontalalignment='center', verticalalignment='bottom')
    ax.add_patch(r)
    
```

```{python}
from imfun import ui
```

```{python}
x = load_stack_with_rescale(names_with_scale[randint(len(names_with_scale))])

proj = x[20:-20].max(0)
projs = ndi.gaussian_filter(proj,3)

pts = img2points(proj, 1)
c = img_center(projs, percentile(projs, 99))



patch_sizes=[16, 32, 64]

figure()
grid(False)
imshow(proj)
ax = gca()

#colormaps = [cm.Reds, cm.Blues, cm.Greens]

for j,ps in enumerate(patch_sizes):
    for i in range(5-j):
        color =  uniform(size=3)
        color[j] = 1
        color[:j] *= 0.75
        color[j+1:] *= 0.75
        x0,y0 = randint(proj.shape[0]-ps),randint(proj.shape[1]-ps)
        r = Rectangle((x0,y0), ps, ps, fc='none',edgecolor=color,lw=3)
        ax.add_patch(r)

add_scalebar(gca(), 10, scale=0.2)
#plot(c[1], c[0], 'ro')
setp(gca(), xticks=[], yticks=[])
```

```{python}
#pts
```

```{python}
# %time hc_results_64 = collect_complexities(64, sigmas_new, need_rebuild=True)
```

```{python}
# %time hc_results_48 = collect_complexities(48, sigmas_new,need_rebuild=True)
```

```{python}
# %time hc_results_32 = collect_complexities(32, sigmas_new,need_rebuild=True)
```

```{python}
# %time hc_results_24 = collect_complexities(24, sigmas_new,need_rebuild=True)
```

```{python}
# %time hc_results_16 = collect_complexities(16, sigmas_new,need_rebuild=True)
```

```{python}
# %time hc_results_12 = collect_complexities(12, sigmas_new,need_rebuild=True)
```

```{python}
# %time hc_results_64r = collect_complexities(64, sigmas_new,randomized=True,need_rebuild=True)
```

```{python}
# %time hc_results_48r = collect_complexities(48, sigmas_new,randomized=True,need_rebuild=True)
```

```{python}
# %time hc_results_32r = collect_complexities(32, sigmas_new,randomized=True,need_rebuild=True)
```

```{python}
# %time hc_results_24r = collect_complexities(24, sigmas_new,randomized=True,need_rebuild=True)
```

```{python}
# %time hc_results_16r = collect_complexities(16, sigmas_new,randomized=True,need_rebuild=True)
```

```{python}
# %time hc_results_12r = collect_complexities(12, sigmas_new,randomized=True,need_rebuild=True)
```

```{python}

```


## Snails


 - [ ] Regularize vector field before running snails?
 - [ ] Tensor-based denoising for these data --> any good?

```{python}
stackz_keep = stackz
```

```{python}
stackz = test_stack
```

```{python}
#start_mask = np.zeros(stackz.shape, bool)
#start_mask[75//2-1:75//2+2, 275//2-1:275//2+2, 275//2-1:275//2+2] = True
#start_mask = np.ones(stackz.shape, bool)
start_mask = stackz > percentile(stackz, 25)
100*np.sum(start_mask)/stackz.size
```

```{python}
reload(hcecp)
```

```{python}
noise_sd = np.std(stackz-ndi.gaussian_filter(stackz,3))
noise_sd
```

```{python}
stackz_noisy = stackz + 2*noise_sd*randn(*stackz.shape)
```

```{python}
sigma = 1.5
```

```{python}
# %time sato, Vf = hcecp.sato3d(stackz_noisy, sigma, hessian_variant='gradient_of_smoothed', return_vectors=True)
```

```{python}
#napari.view_image(sato*(sato > percentile(sato,95)), ndisplay=3, gamma=0.1)
```

```{python}
#noise_sd = np.std(stackz[sato <= percentile(sato,95)])
#noise_sd
```

```{python}
# #%time trails = hcecp.turbosnail_vesselness_food(stackz, 1.0, T=500,mask=start_mask)
tsnail_kw = dict(amp=0.5, vfield_gamma=1, min_counts=10, T=2000, mask=start_mask)
# %time xtrails, vtrails,counts = hcecp.turbosnail_vesselness_food(stackz_noisy, sigma,food_memory=3, **tsnail_kw)
```

```{python}
# %time xtrails_long, vtrails_long,counts_long = hcecp.turbosnail_vesselness_food(stackz_noisy, sigma, food_memory=9, **tsnail_kw)
```

```{python}
sigmas = [0.75, 1.5, 3, 6]
```

```{python}
ms_xtrails = {sigma:hcecp.turbosnail_vesselness_food(stackz_noisy, sigma, food_memory=3, **tsnail_kw) for sigma in tqdm(sigmas)}
```

```{python}
ms_sato = {sigma:hcecp.sato3d(stackz_noisy, sigma,return_vectors=False) for sigma in tqdm(sigmas)}
```

```{python}

```

```{python}
# #%time trails1 = hcecp.turbosnail_vesselness(stackz, 1.0, amp=2, vfield_gamma=0.5, T=500,mask=start_mask)
```

```{python}
loc = 99,95
```

```{python}
np.min(counts)
```

```{python}
figure(figsize=(12,4))
hist(ravel(counts[counts>0]), 200, density=True,log=True,);
#xlim(0,50)
```

```{python}
plot(stackz_noisy[:,loc[0],loc[1]], label='raw')

for s in sigmas:
    plot(ms_xtrails[s][0][:,loc[0],loc[1]],label=s)

#plot(xtrails[:,loc[0],loc[1]],label='short')
#plot(xtrails_long[:,loc[0],loc[1]],label='long')

#plot(vtrails[:,loc[0],loc[1]],label='short')
#plot(vtrails_long[:,loc[0],loc[1]],label='long')


legend()
```

```{python}
napari_kw = dict( gamma=0.75, rendering='mip',blending='additive')
```

```{python}
# #w.add_image?
```

```{python}
# w = napari.view_image(stackz_noisy, ndisplay=3, name='raw', **napari_kw)
# w.add_image(xtrails, colormap='magenta',name='xtrails', **napari_kw)
# w.add_image(xtrails_long, colormap='green',name='xtrails_long', **napari_kw)
# w.add_image(vtrails, colormap='yellow',name='vtrails', **napari_kw)
# #w.add_image(vtrails_long, colormap='cyan', **napari_kw)
# w.add_image(counts, colormap='cyan', **napari_kw)
# w.add_image(sato, colormap='red', **napari_kw)
```

```{python}
#w.close()
```

```{python}
#counts
```

```{python}
colormap_ring = ['yellow','green','magenta','cyan']
```

```{python}
ms_sato_mx = np.max([ms_sato[s]*s**2 for s in sigmas],0)
```

```{python}
w = napari.view_image(stackz_noisy, ndisplay=3, name='raw', **napari_kw)
for s,cm in zip(sigmas,colormap_ring):
    w.add_image(ms_sato[s], colormap=cm, name=f'sato_{s:1.2f}',**napari_kw)
    
w.add_image(ms_sato_mx, colormap='red',name='max_proj',**napari_kw)
w.add_image(stackz_noisy*hcecp.percentile_rescale(ms_sato_mx), colormap='red',name='weighted',**napari_kw)
```

```{python}
ms_trails_mean = np.mean([ms_xtrails[s][0] for s in sigmas],0)
```

```{python}
w2 = napari.view_image(stackz_noisy, ndisplay=3, name='raw', **napari_kw)
for s,cm in zip(sigmas,colormap_ring):
    w2.add_image(ms_xtrails[s][0], colormap=cm, name=f'xtrails_{s:1.2f}', **napari_kw)
    
w2.add_image(ms_trails_mean, colormap='red',name='max_proj',**napari_kw)    
```

```{python}
w3 = napari.view_image(stackz_noisy, ndisplay=3, name='raw', **napari_kw)
for s,cm in zip(sigmas,colormap_ring):
    w3.add_image(ms_xtrails[s][1], colormap=cm, name=f'vtrails_{s:1.2f}',**napari_kw)
```

```{python}
w4 = napari.view_image(stackz_noisy, ndisplay=3, name='raw', **napari_kw)
for s,cm in zip(sigmas,colormap_ring):
    w4.add_image(ms_xtrails[s][1]/(1e-6+ms_xtrails[s][2]), colormap=cm, name=f'vtrails_{s:1.2f}',**napari_kw)
```

```{python}

```

```{python}
w5 = napari.view_image(stackz_noisy, ndisplay=3, name='raw', **napari_kw)
w5.add_image(ms_trails_mean,name='ms_trails', **napari_kw)
w5.add_image(stackz_noisy-ms_trails_mean,name='ms_trails', **napari_kw)
```

```{python}
#napari.view_image(stackz_noisy*hcecp.percentile_rescale(np.max([ms_sato[s]*s**2 for s in sigmas],0)))
```

```{python}

```

```{python}
weights = hcecp.percentile_rescale(sato)
```

```{python}
mask = weights > 0.99
```

```{python}
field = 10*Vf[...,0][...,::-1]*weights[...,None]
```

```{python}
minus_field = -field
```

```{python}
#starts = permutation(array(where(mask)).T)[:min(sum(mask),1)]
starts = np.array([(38, 150, 196),
                   (44,99,153),
                   (41,71,79),
                   (16,163,135),
                  ])
len(starts)
```

```{python}

#trails = hcecp.collect_trails(field, start_mask)
trails = np.zeros(stackz.shape)


kw = dict(dt=0.25, T=100000, m=10, gamma_speed=0.05,friction=0.1, wrap_bounds=False)

for x0 in tqdm(starts):
    hcecp.make_trajectory3d(x0, field, trails, **kw)
    hcecp.make_trajectory3d(x0, minus_field, trails, **kw)
```

```{python}
np.max(trails)
```

```{python}
w = napari.view_image(stackz, ndisplay=3)
w.add_image(log(1+clip(trails,0,10)), colormap='red',blending='additive')
```

```{python}
reload(hcecp)
```

```{python}
ms_sato = {}
```

```{python}
ms_sato[1.5] = hcecp.sato3d(stackz, 1.5)
```

```{python}
# %time trails = hcecp.turbosnail_vesselness(stackz, 1.5, T=1000, dt=0.25, mask=ones(stackz.shape,bool))
```

```{python}
ms_trails = {s:hcecp.turbosnail_vesselness(stackz, s, T=1000, dt=0.25, mask=ones(stackz.shape,bool)) for s in tqdm(sigmas)}
```

```{python}
ms_trails_combined = np.sum([ms_trails[k] for k in [1,2,4]],axis=0)
```

```{python}
sigmas
```

```{python}
w = napari.view_image(stackz, ndisplay=3)
#w.add_image(log(1+clip(trails,0,100)), colormap='red',blending='additive')
#w.add_image(ms_sato[1.5], colormap='green',blending='additive',gamma=0.5)
#w.add_image(ndi.median_filter(trails,3), colormap='magenta',blending='additive',gamma=0.5)
w.add_image(ndi.median_filter(ms_trails[0.5],3), colormap='magenta',blending='additive',gamma=0.5)
w.add_image(ndi.median_filter(ms_trails[1],3), colormap='red',blending='additive',gamma=0.5)
w.add_image(ndi.median_filter(ms_trails[2],3), colormap='green',blending='additive',gamma=0.5)
w.add_image(ndi.median_filter(ms_trails[4],3), colormap='blue',blending='additive',gamma=0.5)
#w.add_image(ndi.median_filter(ms_trails_combined,3), colormap='magenta',blending='additive',gamma=0.5)
```

```{python}
scr = w.screenshot()
imshow(scr)
grid(False)
```

```{python}
napari.view_image(weights==0)
```

```{python}

```

```{python}

```

```{python}

```

```{python}
# %time meta_acc = [ccdb.get_axes(ccdb.read_pic(name)[1]) for name in names]
```

```{python}
spatial_scales = [(name, m[0][0]) if len(m) else (name, None) for name,m in zip(names,meta_acc)]
```

```{python}
spatial_scales
```

```{python}
_ = hist([s[1] for s in spatial_scales], 25)
axvline(0.05, color='r', ls='--')
```

```{python}
target_scale= 0.05 # um/px
```

```{python}
import os

def make_filtered_stack(name, scale, suff = '-fproj'):
    print('Loading z-stack %s.'%name)
    
    out_img_name = name + suff+'.png'
    out_name = name + suff + '.npy'
    
    if os.path.exists(out_name):
        print('skipping an already existing file/n/n')
        return
    
    stack, meta = ccdb.read_pic(name)
    stack = aio.resample_stack(stack, scale, target_scale)
    
    print('Making filtered stack')
    stack_f = enh.get_filtered_stack(stack,njobs=10)
    stack_f = enh.simple_rescale(stack_f, 0.05, 99.95)
    
    
    
    print('Saving output to %s'%out_name)
    
    projection = stack_f.max(0)
    np.save(out_name, projection)

    f,axs = plt.subplots(1,2, figsize=(14,7))
    for ax,st in zip(axs, (stack,stack_f)):
        img = enh.simple_rescale(st.max(0))
        ax.imshow(img, cmap='gray')
    f.tight_layout()
    f.savefig(out_img_name)
    close(f)
    print('/n/n')
    return projection
    
def make_simple_projection(name, scale, suff='-mxproj'):
    print('Loading z-stack %s.'%name)
    
    out_name = name + suff + '.npy'
    
    if os.path.exists(out_name):
        print('skipping an already existing file/n/n')
        return
    
    stack, meta = ccdb.read_pic(name)
    stack = aio.resample_stack(stack, scale, target_scale)
    
    
    print('Saving output to %s'%out_name)
    
    projection = stack.max(0)
    np.save(out_name, projection)
    
```

```{python}

#for name, scale in spatial_scales:
#   make_simple_projection(name,scale)
```

```{python}
# %connect_info
```

```{python}
processed_names = [n for n,_ in spatial_scales]
```

```{python}
projections = [np.load(name+'-fproj.npy') for name in processed_names]
projections_simple = [np.load(name+'-mxproj.npy') for name in processed_names]

cmasks = [np.load(name+'-fproj-cmask.npy') for name in processed_names]
cmasks2 = [np.load(name+'-fproj-cmask2.npy') for name in processed_names]
```

```{python}
len(projections)
```

```{python}
from imfun import ui
```

```{python}
ui.group_maps([p for p,n in zip(projections, processed_names) if '1wk' in n],7,colorbar=False)
tight_layout()
```

```{python}
week_tags = {n.split('-')[0].strip() for n in processed_names}
week_tags
```

```{python}
ui.group_maps([p for p,n in zip(projections, processed_names) if '3wk' in n],colorbar=False)
tight_layout()
```

```{python}

```

```{python}
ui.group_maps([p for p,n in zip(projections, processed_names) if '4wk' in n],colorbar=False)
tight_layout()
```

```{python}
from collections import defaultdict
```

```{python}
from imfun import fseq
```

```{python}
grouped_projs.keys()
```

```{python}
# %matplotlib tk
```

```{python}
for _,_,p in grouped_projs['1wk']:
    p.start()
```

```{python}
close('all')
```

```{python}
imshow(grouped_projs['1wk'][-2][1])
```

```{python}
p = grouped_projs['1wk'][-2][1]
m = p > 0.1*p.max()

```

```{python}
import morphsnakes
```

```{python}
# %matplotlib tk
```

```{python}
def make_cell_mask(p):
    m = p >= 0.1*p.max()
    px = enh.dctsplines.l2spline(p,1)# - enh.dctsplines.l2spline(p, 100)
    px = clip(px, 0.01*px.max(), px.max())
    #px = clip(px, 1e-5, px.max())
    acwe = morphsnakes.MorphACWE(sqrt(px), smoothing=1, lambda1=1, lambda2=10)
    acwe.set_levelset(m)
    acwe.run(1000)
    mask = enh.largest_region(acwe.levelset)
    return mask
```

```{python}
from scipy import ndimage
```

```{python}
def auto_levelset(data):
    #return data > threshold_otsu(data)
    return data > percentile(data, 25)
    #return data > 0.1*np.max(data)
    #return data > percentile(data, 75)
    
    
def make_cell_mask2(p):
    out = np.zeros_like(p)
    sh = p.shape
    p = ndimage.zoom(p, 0.25)
    m = p >= 0.1*p.max()
    
    px = enh.dctsplines.l2spline(p,1)# - enh.dctsplines.l2spline(p, 100)
    px = clip(px, 0.01*px.max(), px.max())
    #px = clip(px, 1e-5, px.max())
    acwe = morphsnakes.MorphACWE(sqrt(px), smoothing=3, lambda1=1, lambda2=5)
    acwe.set_levelset(auto_levelset(px))
    acwe.run(1000)
    mask = enh.largest_region(acwe.levelset)
    mask =  ndimage.zoom(mask.astype(float32),4)>0.5
    sh2 = mask.shape
    crop = ([slice(min(sh_,sh2_)) for sh_,sh2_ in zip(sh, sh2)])
    out[crop] = mask[crop]
    return out.astype(bool)
```

```{python}
from imfun import ui
```

```{python}
k = 57
p = projections[k]
px = enh.simple_rescale(projections_simple[k])
# #%time mask1 = make_cell_mask(p)
# %time mask2 = make_cell_mask2(px)
print(p.shape, mask2.shape)
ui.group_maps([px,p, mask1,mask2],colorbar=False, imkw=dict(cmap='plasma'),figscale=7)
gcf()
```

```{python}
figure(figsize=(12,12)); imshow(projections[k]); 
contour(mask2, levels=[0.5],colors='r')
```

```{python}
close('all')
```

```{python}
argmax([p.shape[0] for p in projections])
```

```{python}
px = enh.dctsplines.l2spline(p,1)
px = clip(px, 0.01*px.max(), px.max())

# #%time px = enh.dctsplines.l2spline(p,1)-enh.dctsplines.l2spline(p, 100)
#px = clip(px, 1e-5, px.max())
```

```{python}
imshow(sqrt(px))
```

```{python}
#imshow(log(px)); 
imshow(p)
contour(mask, levels=[0.5], colors=['r'])
#contour(enh.largest_region(acwe.levelset), levels=[0.5], colors=['r'])
#gcf()
```

```{python}
# %matplotlib inline
```

```{python}
# %time cmasks = [make_cell_mask(p) for p in projections]
```

```{python}
# %time cmasks2 = [make_cell_mask2(enh.simple_rescale(p)) for p in projections_simple]
```

```{python}
from collections import defaultdict
```

```{python}
grouped_projs = defaultdict(list)

for proj,cmask,name in zip(projections,cmasks2,processed_names):
    tag = name.split('-')[0].strip()
    #picker = ui.Picker(fseq.from_array(array([proj]*10)))
    grouped_projs[tag].append((name, proj, cmask))
```

```{python}
def show_cells(key,figscale=6):
    nelements = len(grouped_projs[key])
    nrows,ncols = ui.plots.guess_gridshape(nelements)
    figsize = (figscale*ncols,figscale*nrows)
    f,axs = subplots(nrows,ncols,figsize=figsize)
    
    for (name, proj, cmask),ax in zip(grouped_projs[key],ravel(axs)):
        ax.imshow(proj)
        ax.contour(cmask, levels=[0.5],colors='r')
        setp(ax, 'xticks', [], 'yticks', [],  'frame_on', False)
    for ax in ravel(axs)[nelements:]:
        setp(ax, visible=False)
    tight_layout()
```

```{python}
show_cells('1wk')
```

```{python}
show_cells('3wk')
```

```{python}
show_cells('4wk')
```

**NB!** other ideas for masks:

 0. Use simple projections (with clipped intensity) for masks
 1. First downsample, then run with `smooth=2`, then upsample the mask
 2. Also make and use masks for thick branches and soma

